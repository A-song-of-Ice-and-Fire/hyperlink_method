{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该脚本用来确定数据集的联合抽样点，联合抽样点是超边的度，任取一条超边，记大小为X，联合抽样点是满足P(X>=x)>=0.2的最大值x_max。为了方便分析，取下界3和上界11\n",
    "# 即 min(max(3,argmax(P(X>=x)>=0.2)),11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"../dataset\"\n",
    "split_area = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv_cond-mat [8.186e+03 5.239e+03 2.304e+03 9.930e+02 4.860e+02 2.680e+02 1.590e+02\n",
      " 8.700e+01 4.900e+01 2.900e+01 1.600e+01 8.000e+00 6.000e+00 3.000e+00\n",
      " 3.000e+00 0.000e+00 1.000e+00]\n",
      "cat-edge-music-blues-reviews [86. 52. 39. 33. 38. 37. 25. 25. 19. 20. 13. 18. 19. 22. 14. 20. 13. 15.\n",
      "  9. 11. 17.  8.  4.  7. 14.  4.  8. 10. 11.  5.  2.  4.  6.  0.  6.  3.\n",
      "  2.  4.  1.  4.  2.  3.  0.  4.  1.  2.  4.  2.  1.  0.  2.  2.  3.  1.\n",
      "  1.  1.  0.  2.  2.  1.  0.  0.  1.  1.  0.  1.  1.  1.  0.  0.  1.  1.\n",
      "  0.  0.  1.  0.  1.  2.  0.  0.  0.  1.]\n",
      "chuancai [248.  71.  28.   9.   7.   5.   1.   2.   3.   2.   1.]\n",
      "coauth-DBLP [3.2064e+04 3.5381e+04 2.5994e+04 1.4512e+04 7.2060e+03 3.1270e+03\n",
      " 1.5540e+03 8.0900e+02 4.4300e+02 2.6300e+02 1.7100e+02 1.1400e+02\n",
      " 8.3000e+01 5.3000e+01 4.0000e+01 2.8000e+01 2.9000e+01 2.2000e+01\n",
      " 2.1000e+01 2.1000e+01 1.2000e+01 1.2000e+01 5.0000e+00 1.2000e+01]\n",
      "crime [197.  94.  51.  29.  11.   8.   8.   3.   0.   1.   1.   3.   0.   1.\n",
      "   0.   0.   1.]\n",
      "email-Eu [13188.  5080.  2357.  1401.   913.   564.   365.   282.   193.   137.\n",
      "   112.    75.    72.    66.    54.    47.    55.    47.    40.    32.\n",
      "    29.    20.    15.    19.]\n",
      "github [1.6108e+04 6.4750e+03 3.4940e+03 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      "iAB_RBC_283 [ 63.  16. 109. 149.  12.  13.   0.   4.]\n",
      "iJO1366 [430. 129. 674. 581. 196. 166.  23.  10.   1.   1.   2.   1.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   1.]\n",
      "walmart-trips [12838. 10110.  7651.  5792.  4527.  3640.  3081.  2574.  2170.  1860.\n",
      "  1688.  1517.  1260.  1145.  1013.   864.   754.   667.   659.   571.\n",
      "   521.   425.   387.   265.]\n",
      "yuecai [273.  87.  30.  18.  10.   1.   5.   0.   1.]\n",
      "DBLP-Co-authorship [179. 331. 447. 411. 431. 298. 189. 116.  53.  36.  12.  14.   8.  14.\n",
      "   6.   9.   2.   2.   1.   0.   1.   0.   0.   0.   1.]\n",
      "Cora-Co-citation [572. 447. 306. 178.]\n",
      "Cora-Co-reference [309. 169.  98.  90.  45.  28.  28.  23.  10.  13.   7.   7.   3.   7.\n",
      "   5.   4.   3.   6.   1.   0.   1.   1.   0.   1.   0.   1.   2.   1.\n",
      "   1.   1.   3.   0.   0.   0.   0.   0.   1.   0.   0.   1.   1.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.]\n",
      "CoreComplex [361. 411. 158.  95.  46.  43.  27.  16.  24.  14.  17.  18.   8.   9.\n",
      "  11.   5.   2.   3.   2.   0.   2.   1.   2.   2.   1.   2.   1.   0.\n",
      "   2.   1.   1.   1.   0.   0.   1.   1.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   1.   0.   1.   1.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   1.]\n"
     ]
    }
   ],
   "source": [
    "se = pd.Series(dtype=int)\n",
    "for dataset_dir in os.listdir(input_dir):\n",
    "    if len(dataset_dir.split(\".\")) >= 2:\n",
    "        continue\n",
    "    for file_name in os.listdir(os.path.join(input_dir,dataset_dir)):\n",
    "        suffix = file_name.split(\".\")[-1]\n",
    "        if suffix not in [\"npy\",\"npz\"]:\n",
    "            continue\n",
    "        input_path = os.path.join(input_dir,dataset_dir,file_name)\n",
    "        if suffix == \"npy\":\n",
    "            inc_matrix = np.load(input_path).astype(np.int32)\n",
    "            edge_size_vector = inc_matrix.sum(axis=0)\n",
    "        elif suffix == \"npz\":\n",
    "            inc_matrix = ssp.load_npz(input_path).tocsr().astype(np.int32)\n",
    "            edge_size_vector = np.asarray(inc_matrix.sum(axis=0)).squeeze()\n",
    "        max_base = edge_size_vector.max()\n",
    "        dist_vector = np.zeros(max_base-2+1,dtype=np.float32)\n",
    "        for value in range(2,max_base+1):\n",
    "            dist_vector[value-2] = (edge_size_vector == value).sum()\n",
    "        #dist_vector /= dist_vector.sum()\n",
    "        print(dataset_dir,dist_vector)\n",
    "        # 接下来 获得分位点\n",
    "        for x in range(11,2,-1):\n",
    "            if dist_vector[x-2:].sum() >= 100: #split_area:\n",
    "                break\n",
    "        se.loc[dataset_dir] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arXiv_cond-mat                  10\n",
       "cat-edge-music-blues-reviews    11\n",
       "chuancai                         3\n",
       "coauth-DBLP                     11\n",
       "crime                            4\n",
       "email-Eu                        11\n",
       "github                          11\n",
       "iAB_RBC_283                      5\n",
       "iJO1366                          7\n",
       "walmart-trips                   11\n",
       "yuecai                           3\n",
       "DBLP-Co-authorship              11\n",
       "Cora-Co-citation                 5\n",
       "Cora-Co-reference                9\n",
       "CoreComplex                     11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ae48f9685e5b2af60d1728174d9b753bd539071809e94d6c68e99c51f77af79"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('hlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
